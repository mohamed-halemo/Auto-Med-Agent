#  Auto-Med AI Agent â€“ Generative Medical Research Assistant

> A generative AI agent that reads medical research papers, answers domain-specific questions, summarizes papers, performs PubMed searches, handles calculations, and learns from previous conversations.

---

## ğŸ“Œ Project Highlights

- ğŸ§¾ **Literature-Aware RAG Agent** using txt data
- ğŸ”§ **Tools Integration**: PubMed search, clinical trials search
- ğŸ“š **Automatic Question Generation** from research papers
- ğŸ“ **Evaluation** with ROUGE + BLEU
- ğŸ’¡ **Powered by**: HuggingFace Transformers, Sentence Transformers, Streamlit, FAISS

---

## ğŸ—ï¸ System Architecture

graph TD

    A[User Input] --> B{Query Type?}
    B -->|Medical Q/A| C[RAG Agent (PDF / Abstracts)]
    B -->|clinical trial| D[Clinical trial search tool]
    B -->|PubMed| E[PubMed Search Tool]
    C --> F[Answer + Summary]
    D --> F 
    E --> F
    G --> A
---


| Layer         | Tools Used                                   |
| ------------- | -------------------------------------------- |
| Frontend      | Streamlit                                    |
| Backend Logic | Python + LangChain-like agent design         |
| NLP Models    | Sentence Transformers (MiniLM), HF Pipelines |
| Tools         | Custom Python tools (PubMed,cLINICAL TRIALS)           |
| Evaluation    | ROUGE, BLEU                                  |

---

git clone https://github.com/mohamed-halemo/Auto-Med-Agent
cd automed-ai-agent

# Install requirements
pip install -r requirements.txt

# Run the app
```bash
streamlit run app.py

automed-ai-agent/
â”‚
â”œâ”€â”€ app.py                         # Streamlit frontend
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ literature_agent.py       # Handles RAG with summarization
â”‚   â””â”€â”€ tool_agent.py             # Routes queries to tools or literature
â”‚
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ toolkit.py                # Calculator, PubMed search
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ generate_questions.py     # Uses QA pipeline to generate test set
â”‚   â””â”€â”€ evaluate_answers.py       # BLEU + ROUGE evaluations
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ generated_qas.json        # Auto-generated Q/A for evaluation
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ helper.py                 # PDF & text loading utilities
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
# How It Works
> Upload a PDF or enter a PubMed topic.

> The agent creates document embeddings and builds a FAISS retriever.

> You can ask:

> Domain questions â†’ retrieved + summarized


> pubmed cancer â†’ returns top IDs

> Memory tracks past Q&A to allow follow-ups.

> Metrics + Evaluation
> Generated 20 Q/A pairs from documents using Hugging Face's transformers pipeline.

> Evaluated answers using nltk BLEU and rouge_score metrics.

> Used this to benchmark the effectiveness of the current RAG setup.

```
User query
     â†“
ToolUsingAgent.run()
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ "clinical"            â”‚   "pubmed"    â”‚
 â–¼                       â–¼
clinical_trial_search()  pubmed_search()
                                             â†“
                                     LiteratureAgent.run()
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚ Semantic     â”‚
                                       â”‚ Search (FAISSâ”‚
                                       â”‚ + Bi-Encoder)â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â†“
                                   Cross-Encoder Re-rank
                                       â†“
                                  QA Model (extractive)
                                       â†“
                                 Summarization Model
                                       â†“
                              Best Answer + Summary Returned


# By Mohamed Hafez
