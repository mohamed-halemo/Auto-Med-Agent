# ðŸ§  AutoMed AI Agent â€“ Generative Medical Research Assistant

> A generative AI agent that reads medical research papers, answers domain-specific questions, summarizes papers, performs PubMed searches, handles calculations, and learns from previous conversations.

---

## ðŸ“Œ Project Highlights

- ðŸ§¾ **Literature-Aware RAG Agent** using PubMed articles (PDF or Abstracts)
- ðŸ§  **Conversational Memory** to retain context
- ðŸ”§ **Tools Integration**: PubMed search, calculator
- ðŸ“š **Automatic Question Generation** from research papers
- ðŸ“ **Evaluation** with ROUGE + BLEU
- ðŸ’¡ **Powered by**: HuggingFace Transformers, Sentence Transformers, Streamlit, FAISS

---

## ðŸ—ï¸ System Architecture

graph TD

    A[User Input] --> B{Query Type?}
    B -->|Medical Q/A| C[RAG Agent (PDF / Abstracts)]
    B -->|Math| D[Calculator Tool]
    B -->|PubMed| E[PubMed Search Tool]
    C --> F[Answer + Summary]
    D --> F
    E --> F
    F --> G[Conversation Memory]
    G --> A
---


| Layer         | Tools Used                                   |
| ------------- | -------------------------------------------- |
| Frontend      | Streamlit                                    |
| Backend Logic | Python + LangChain-like agent design         |
| NLP Models    | Sentence Transformers (MiniLM), HF Pipelines |
| Tools         | Custom Python tools (PubMed, Math)           |
| Memory        | Session-based Memory                         |
| Evaluation    | ROUGE, BLEU                                  |

---

git clone https://github.com/mohamed-halemo/Auto-Med-Agent
cd automed-ai-agent

# Install requirements
pip install -r requirements.txt

# Run the app
```bash
streamlit run app.py

automed-ai-agent/
â”‚
â”œâ”€â”€ app.py                         # Streamlit frontend
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ literature_agent.py       # Handles RAG with summarization
â”‚   â””â”€â”€ tool_agent.py             # Routes queries to tools or literature
â”‚
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ toolkit.py                # Calculator, PubMed search
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ generate_questions.py     # Uses QA pipeline to generate test set
â”‚   â””â”€â”€ evaluate_answers.py       # BLEU + ROUGE evaluations
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ generated_qas.json        # Auto-generated Q/A for evaluation
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ helper.py                 # PDF & text loading utilities
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
# How It Works
> Upload a PDF or enter a PubMed topic.

> The agent creates document embeddings and builds a FAISS retriever.

> You can ask:

> Domain questions â†’ retrieved + summarized

> calculate 7+3 â†’ calculator tool

> pubmed cancer â†’ returns top IDs

> Memory tracks past Q&A to allow follow-ups.

> Metrics + Evaluation
> Generated 20 Q/A pairs from documents using Hugging Face's transformers pipeline.

> Evaluated answers using nltk BLEU and rouge_score metrics.

> Used this to benchmark the effectiveness of the current RAG setup.

